<!DOCTYPE HTML>
<!--
	Magnetic by Pixelarity
	pixelarity.com | hello@pixelarity.com
	License: pixelarity.com/license
-->
<html>

<head>
	<title>Building Decision Trees with ID3 | The Round Pegs and Square Holes of Big Data</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<!-- @stylesheet main -->
	<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
</head>

<body id="top">

	<!-- Page Wrapper -->
	<div id="page-wrapper">

		<!-- Header -->
		<header id="header">
			<h1><a href="../../">Big Data <span>The Round Pegs and Square Holes</span></a></h1>
			<nav>
				<a href="#menu">Menu</a>
			</nav>
		</header>

		<!-- Menu -->
		<nav id="menu">
			<div class="inner">
				<h2>Menu</h2>
				<ul class="links">
					<li><a href="../../">Home</a></li>
					<li><a href="../../trees/">Decision Trees</a></li>
					<li><a href="../../pre/dimensionality/">Dimensionality Reduction</a></li>
					<li><a href="../../proc/id3/">Building The Tree</a></li>
					<li><a href="../../post/performance/">Performance</a></li>
					<li><a href="../../post/forests/">Random Forests</a></li>
					<li><a href="../../casestudy/">Case Study</a></li>
					<li><a href="../../post/boosting/">Boosting</a></li>
				</ul>
			</div>
		</nav>

		<!-- Main -->
		<div id="main">

			<!-- Content -->
			<section id="content" class="default three">
				<header class="major">
					<h2>Building Decision Trees with ID3</h2>
					<p><strong>Iterative Dichotomiser 3</strong> (ID3) is an algorithm invented by Ross Quinlan and is used to <strong>generate decision trees</strong> from a dataset.</p>
				</header>
				<div class="content">
					<h3>Entropy</h3>
					<p>
						To understand how to build a decision tree, we must have a way of quantifying an amount of information. This is known as <strong>entropy</strong>.
					</p>
					<p>
						In a decision tree, the question can have many possible outcomes but, for simplicity, we will use a binary decision tree in this example. Imagine a machine that outputs the letters A, B, C, or D in the proportions shown in <strong>Figures 1 and 2</strong>. We wish to establish what the next letter is by repeatedly evaluating conditions. Although the first tree has a lesser total depth, each node is at depth 2 and the average number of questions asked (conditions evaluated) is 2. By contrast, in the second tree the average number of conditions evaluated is <span class="maths">1.75 (= 0.5*1 + 0.25*2 + 0.125*3 + 0.125*3)</span>. Thus, tree 2 will on average perform better at identifying the next letter.
					</p>

					<figure class="image figure">
						<img src="../../images/proc/id3/fig1.svg" />
						<figcaption>
							<strong>Figure 1: </strong> this tree's total depth is lower than Figure 2, but it will still perform poorer due to each outcome's probabilities
						</figcaption>
					</figure>

					<figure class="image figure">
						<img src="../../images/proc/id3/fig2.svg" />
						<figcaption>
							<strong>Figure 2: </strong> this tree's total depth is higher than Figure 1, but it will perform better, as deeper levels have lower probabilities
						</figcaption>
					</figure>

					<p>
						Using this method, we can calculate the entropy (a measure of uncertainty) of a system as the sum of the probability of the outcome multiplied by the number of questions required to reach the outcome.
					</p>

					\[H = \sum\limits_{i=1}^{n} p * number\space of \space questions \]

					<p>
						In a well created tree, this is equal to <cite>(1)</cite>:
					</p>

					\[H = \sum\limits_{i=1}^{n} p_{i} * log_{2}(\frac{1}{p_{i}}) \]

					<hr/>

					<h3>Building A Tree</h3>
					<p>
						In order to build a tree, we need a particular attribute from the data set to be our target attribute (the player position in the <a href="../../trees/">basketball example</a>). The algorithm for creating a tree is recursive as each time you split on a certain attribute, you create a new tree beneath it for each of the possible outcomes.
					</p>

					<h3>Pseudocode <cite>(2)</cite></h3>
					<pre><code>ID3 (Examples, Target_Attribute, Attributes)
    Create a root node for the tree
   //bases cases
    If all examples are positive, Return the leaf, with label = +.
    If all examples are negative, Return the leaf, with label = -.
   
   
If number of predicting attributes is empty, then Return the leaf,
    with label = most common value of the target attribute in the examples.
   
//recursive call
Otherwise Begin
        A = The Attribute that best classifies examples.
        Decision Tree attribute for Root = A.
        For each possible value, vi, of A,
            Add a new tree branch below Root, corresponding to the test A = vi.
            Let Examples(vi) be the subset of examples that have the value vi for A
            If Examples(vi) is empty
                Then below this new branch add a leaf node with label = most common target value in the examples
            Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes – {A})
    End
    Return Root
</code></pre>
					<p>
						Note that it is not possible to always provide a definitive decision based off the training data. On line 7, there are still varying decisions in the training data but no more attributes left to separate them by. As such, the code will select the most common decision.
					</p>

					<h3>Finding the attribute to split on using entropy</h3>
					<p>
						The attribute to split on is the one that will provide the largest information gain. Information gain, IG, is the difference in entropy in the current data before and after splitting on a particular attribute. Information gain can be informally expressed as the amount of uncertainty that was removed by splitting the set on a particular attribute.
					</p>

					<h3>Improvements to the algorithm:</h3>
					<p>
						There are a few other algorithms to build trees that provide marginally better performance (such as C4.5 or C5.0) but these are merely refinements of ID3 and use the same underlying principles <cite>(3)</cite>.
					</p>

					<h3>Problems with ID3</h3>
					<p>
						As with all algorithms, ID3 is not infallible and, with a poor training data set, you will suffer from GIGO (Garbage In, Garbage Out). With a perfect data set, you would get perfect results from a decision tree built using ID3. However, data sets in the real world are never perfect and will contain many outliers, missing variables or even <a href="../../pre/dimensionality/">too many variables</a>. Also, if the data set is too small, you will suffer from <a href="../../post/performance/">overfitting</a>.
					</p>

					<footer class="references">
						<h2>References</h2>
						<p>1. Weisstein EW. Wolfram MathWorld. [Online]. [cited 2017 February 25. Available from: http://mathworld.wolfram.com/Entropy.html.</p>
						<p>2. Quinlan R. Induction of Decision Trees. Machine Learning. 1986 March: p. 81–106.</p>
						<p>3. Peng W. University of New South Wales. [Online]. [cited 2017 02 14. Available from: http://web.arch.usyd.edu.au/~wpeng/DecisionTree2.pdf.</p>
					</footer>
					
				</div>
			</section>

		</div>

		<!-- Footer -->
		<section id="footer">
			<div class="copyright">
				<p>
					Copyright &copy;
					<script type="text/javascript">
						document.write(new Date().getFullYear());
					</script>
					Z. Amjad, R. Padmanabhan, R. Pritchard and G. Zhelev –
					<a href="../../acknowledgements/">Acknowledgements</a>
				</p>
			</div>
		</section>

	</div>

	<!-- Scripts -->
	<script src="../../assets/js/jquery.min.js"></script>
	<script src="../../assets/js/skel.min.js"></script>
	<script src="../../assets/js/util.js"></script>
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="../../assets/js/main.js"></script>

</body>

</html>