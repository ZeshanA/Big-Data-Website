<!DOCTYPE HTML>
<!--
	Magnetic by Pixelarity
	pixelarity.com | hello@pixelarity.com
	License: pixelarity.com/license
-->
<html>
	<head>
		<title>Digits Case Study | The Round Pegs and Square Holes of Big Data</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel='stylesheet' href='../assets/sass/main.css'>
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body id="top">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="../">Big Data <span>The Round Pegs and Square Holes</span></a></h1>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<div class="inner">
							<h2>Menu</h2>
							<ul class="links">
								<li><a href="../">Home</a></li>
								<li><a href="../trees/">Decision Trees</a></li>
								<li><a href="../pre/dimensionality/">Dimensionality Reduction</a></li>
								<li><a href="../proc/id3/">Building The Tree</a></li>
								<li><a href="../post/performance/">Performance</a></li>
								<li><a href="../post/forests/">Random Forests</a></li>
								<li><a href="../casestudy/">Case Study</a></li>
								<li><a href="../post/boosting/">Boosting</a></li>
							</ul>
						</div>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="default four">
								<header class="major">
									<h2>Digits Case Study</h2>
									<p>This demonstrates a <strong>poorly performing</strong> decision tree, identifies the <strong>problem</strong>, and fixes it with the use of <strong>random forests</strong>.</p>
									<a href="https://github.com/rohanpritchard/bigdataroundpegs" class="button big">
										View Code on Github
										<span class="icon fa-angle-right"></span>
									</a>
								</header>
								<div class="content">
									<p>
										Let’s look at an example of a decision tree whose performance is not optimal because it’s flaws are being exacerbated by misuse. We start with a machine learning model which determines the value of a handwritten digit. This model is a basic decision tree classifier.
									</p>
									<p>
										It is built to consider 8x8 black and white images of digits, in the form of 64 dimensioned data points (a light intensity value for each pixel). Here’s a sample of a few of the data points:
									</p>

									<figure class="image figure wide">
										<img src="../images/casestudy/fig1.png" />
										<figcaption>
											<strong>Figure 1: </strong>
											a sample of handwritten digits in the dataset
									</figure>

									<p>
										When we train this model, we do what we always do by splitting the data in half, one for training and one for testing. The following code counts how many of the test data points the model correctly classified:
									</p>

									<pre><code>guessAnswerPair = zip(results, answers)
correct = 0
total = 0

for (guess, answer) in guessAnswerPair:
   if guess==answer:
       correct+=1
   total+=1</code></pre>			
   									<a href="https://github.com/rohanpritchard/bigdataroundpegs" class="button fit special">View Code on Github <span class="icon fa-angle-right"></span></a>
   									
   									<p>
   										When we run the test, we correctly classify 734 out of 898 images, that’s an 82% success rate. This isn’t bad at all, but we can likely improve on this. To do so we should consider what we know about the weaknesses of decision trees, and what we know about the data that the model is learning from, this way we can make modifications to our model to improve it.
   									</p>
   									<p>
   										We know that decision trees suffer from high variance, which means they’ll likely overfit to noisy data. So here comes the part that requires a bit of analysis, considering the weaknesses of our model, is the data likely to exacerbate these flaws?
   									</p>
   									<p>
   										Let’s start by looking at how much data we have; in total our ‘digits’ dataset contains 1797 entries. Unfortunately in this example we don’t know how the data was collected (for instance were random people asked online to draw them with their mouse or were they highly skilled calligraphers). The origins of the dataset might have been useful because it would have indicated how ‘noisy’ the data was. By this we mean how often do we have outliers in our data that stray from our typical expectation.
   									</p>
   									<p>
   										We can however, examine random samples of the data and get a good idea this way. From looking through the data with the data visualiser (uses matplotlib, code available on github), it is quite clear that the data that we’re using is a mishmash of handwriting, ranging from well defined digits, to frankly, something that looks closer to chinese. What this tells us, is that considering the fact that decision trees suffer from overfitting, this model could be fitting to the wrong pieces of data.
   									</p>
   									<p>
   										Fortunately, we’ve just discussed reducing overfitting with decision trees, and the solution was random forests. With random forests, for every odd ‘bad 2’ that a decision tree will use as a classifier, like this one:
   									</p>

   									<figure class="image figure wide">
   										<img src="../images/casestudy/fig2.png" />
   										<figcaption>
   											<strong>Figure 2: </strong>
   											a sample of a poorly written "2"
   									</figure>

   									<p>
   										For which clearly, many ‘3’s may look similar to (and thus does not well generalise what a 2 will look like), there will be a bunch more decision trees that never saw this 2 to generalise from in the first place, and so it will be outweighed.
   									</p>
   									<p>
   										So if we adapt our model to learn with random forests instead, and run the same tests with the same training data, this time we correctly identify 823 of 898, a success rate of 92%. This case study shows how important analysis of the performance of your current model, as well as the nature of the dataset is, for improving effectiveness.
   									</p>

   									<footer class="references">
   										<h2>References</h2>
   										<p>1. Dataset available at SciKit: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html</p>
   									</footer>
   									
								</div>
							</section>

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="copyright">
							<p>
								Copyright &copy; 
								<script type="text/javascript">
								  document.write(new Date().getFullYear());
								</script>
							 	Z. Amjad, R. Padmanabhan, R. Pritchard and G. Zhelev – 
							 	<a href="../acknowledgements/">Acknowledgements</a>
							 </p>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/skel.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../assets/js/main.js"></script>
			<script type="text/javascript">
				hljs.initHighlightingOnLoad();
			</script>

	</body>
</html>