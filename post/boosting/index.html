<!DOCTYPE HTML>
<!--
	Magnetic by Pixelarity
	pixelarity.com | hello@pixelarity.com
	License: pixelarity.com/license
-->
<html>
	<head>
		<title>Boosting | The Round Pegs and Square Holes of Big Data</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<!-- @stylesheet main -->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body id="top">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="../../">Big Data <span>The Round Pegs and Square Holes</span></a></h1>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<div class="inner">
							<h2>Menu</h2>
							<ul class="links">
								<li><a href="../../">Home</a></li>
								<li><a href="../../trees/">Decision Trees</a></li>
								<li><a href="../../pre/dimensionality/">Dimensionality Reduction</a></li>
								<li><a href="../../proc/id3/">Building The Tree</a></li>
								<li><a href="../../post/performance/">Performance</a></li>
								<li><a href="../../post/forests/">Random Forests</a></li>
								<li><a href="../../casestudy/">Case Study</a></li>
								<li><a href="../../post/boosting/">Boosting</a></li>
							</ul>
						</div>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="default four">
								<header class="major">
									<h2>Boosting</h2>
									<p>Boosting <strong>combines</strong> many <strong>weak classifiers</strong> to produce a single, <strong>strong classifier</strong>.</p>
								</header>
								<div class="content">
									<p>
										Boosting is an effective method of producing a very accurate prediction rule by combining relatively inaccurate rules of thumb <cite>(4)</cite>. Obtaining these rules of thumb, also called ‘weak learners’ should be fairly easy - a program that given a set of examples searches for simple prediction rules that could be rough and inaccurate would be sufficient. The two main problems with boosting are how should one extract the most useful rules of thumb from a dataset and once many have been collected, how can they be combined into a single, highly accurate rule?
									</p>
									<p>
										For the latter, the obvious approach of obtaining a combined rule via a vote of the predictions of the rules of thumb turns out to be a good one. For the first question, a good approach would be to focus the 'weak learning' program on the examples that have the highest error rate with the current rules.
									</p>
									<p>
										As an example for boosting, we will look at Adaptive Boosting, or AdaBoost for short. It’s a machine learning meta-algorithm (an algorithm that may provide a sufficiently good solution to an optimisation problem, especially with incomplete or imperfect information). It is most commonly combined with other learning algorithms when their output (‘weak learners’) are combined into a weighted sum that represents the final output of the boosted classifier <cite>(3)</cite>. However, AdaBoost is susceptible to ‘noisy’ data and outliers and is prone to overfitting. 
									</p>
									<p>
										Most learning algorithms perform better on a particular problem type than others and therefore have a lot of different parameters and configurations to get the best performance on the existing dataset. AdaBoost with decision trees is considered to be one of the best overall classifiers. A disadvantage, however, is that AdaBoost is very susceptible to ‘noisy’ data <cite>(1)</cite>.
									</p>
									<p>
										It’s important to note that a boosting algorithm uses other learning algorithms (decision trees, in our case) and therefore can only improve results by manipulating the data, rather than being a better algorithm itself.
									</p>
									
									<h3>The Algorithm</h3>

									<figure class="image figure medium">
										<img src="../../images/post/boosting/fig1.png" />
										<figcaption>
											<strong>Figure 1: </strong>
											pseudocode for AdaBoost <cite>(2)</cite>
										</figcaption>
									</figure>

									<p>
										As we’ve already said, the idea behind a boosting algorithm is to choose training examples for the base learner in a way that would force the learner to infer something new every time it’s called. This is done by selecting training sets on which the rules we have so far would perform very poorly, even worse than their already weak performance. We expect a new classifier which would be different from the others because while the learning algorithm outputs weak learners, it’s still to be expected that it would output classifiers whose predictions are nontrivial. 
									</p>

									<p>
										The weak learner would try to choose some weak hypothesis <span class="maths">h</span> with a low weighted error <span class="maths">e</span>. We don’t expect this error to be small; we just expect it to be a bit better than random (if it’s a random guess, then <span class="maths">e</span> is bounded by 0.5. Therefore each <span class="maths">e</span> would be equal to <span class="maths">½ - x</span> for some small positive constant <span class="maths">x</span>).
									</p>

									<p>
										Once the base classifier h has been returned, the algorithm chooses a parameter <span class="maths">a</span>, as shown in <strong>Figure 1</strong>. <span class="maths">a</span>  measures the importance that is assigned to <span class="maths">h</span>. It’s important to note that <span class="maths">a > 0</span> if <span class="maths">e < ½</span>, and that <span class="maths">a</span> is inversely correlated to <span class="maths">e</span>. Then the distribution is updated as shown. The rule for updating tends to concentrate the weight on the ‘harder’ examples. The effect of it is a new distribution on which the last base classifier h is sure to do poorly. It can be shown that the error of <span class="maths">h</span> with respect to the new distribution is exactly <span class="maths">½</span>,  which is essentially random guessing. This way, AdaBoost forces the base learner to learn new things about the data <cite>(2)</cite>.
									</p>

									<p>
										In the end, AdaBoost combines all the base classifiers into a single final classifier. It’s accomplished by a weighted sum of the base classifiers, where every hypothesis <span class="maths">h</span> is weighted by its parameter <span class="maths">a</span>.
									</p>

									<h3>An example of AdaBoost</h3>
									<p>
										Let’s imagine the following situation: the admission committee for a university has 6 members: A, B, C, D, E and F, who have to decide who’ll succeed at their university. In the start, they just vote yes or no, but after a couple of years, they have enough information to weigh everyone’s judging ability.
									</p>
									<p>
										Now, let’s say that A has a 60% success in predicting. We’ll weigh her vote up, because her guess is better than random. B also has a high success rate, but B’s answers seem to be correlated to A.
									</p>
									<p>
										On the other hand, C has a low success rate, but appears to be looking at different criteria than A and B. While C has less predictive power, if we combine it with A, we will get a stronger classifier than we would by combining A and B.
									</p>
									<p>
										Maybe D is better at guessing who will NOT succeed in university. That’s completely fine, because if D’s decisions are not random, they still provide information.
									</p>
									<p>
										And this goes on until all the classifiers are weighed. Instead of summing A + B + C + D + E + F equally, we weigh these weak classifiers and produce a stronger overall model.
									</p>

									<footer class="references">
										<h2>References</h2>
										<p>
											1. Lok Kei Long, <em>Analyzing big data with decision trees</em>, Master’s thesis, San José State University, 2014 http://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1368&context=etd_projects
										</p>
										<p>
											2. Robert E. Schapire, <em>Boosting: Foundations and Algorithms</em>, 2012
										</p>
										<p>
											3. Yoav Freund, A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, <em>Journal of Computer and System Sciences</em>, 1996, 55, p.119-139
										</p>
										<p>
											4. Yoav Freund &amp; Robert E. Schapire, A Short Introduction to Boosting, <em>Journal of Japanese Society for Artificial Intelligence</em>, 1999, 14(5), p. 771-780
										</p>
									</footer>
								</div>
							</section>

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="copyright">
							<p>
								Copyright &copy; 
								<script type="text/javascript">
								  document.write(new Date().getFullYear());
								</script>
							 	Z. Amjad, R. Padmanabhan, R. Pritchard and G. Zhelev –
							 	<a href="../../acknowledgements/">Acknowledgements</a>
							 </p>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>

	</body>
</html>