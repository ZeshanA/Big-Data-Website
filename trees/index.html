<!DOCTYPE HTML>
<!--
	Magnetic by Pixelarity
	pixelarity.com | hello@pixelarity.com
	License: pixelarity.com/license
-->
<html>
	<head>
		<title>Decision Trees | The Round Pegs and Square Holes of Big Data</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<!-- @stylesheet main -->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body id="top">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="../">Big Data <span>The Round Pegs and Square Holes</span></a></h1>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<div class="inner">
							<h2>Menu</h2>
							<ul class="links">
								<li><a href="../">Home</a></li>
								<li><a href="../trees/">Decision Trees</a></li>
								<li><a href="../pre/dimensionality/">Dimensionality Reduction</a></li>
								<li><a href="../proc/id3/">Building The Tree</a></li>
								<li><a href="../post/performance/">Performance</a></li>
								<li><a href="../post/forests/">Random Forests</a></li>
								<li><a href="../casestudy/">Case Study</a></li>
								<li><a href="../post/boosting/">Boosting</a></li>
							</ul>
						</div>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="default">
								<header class="major">
									<h2>Decision Trees</h2>
									<p>A Decision Tree is a graph that allows you to reach an outcome by repeatedly evaluating conditions.</p>
								</header>
								<div class="content">
									<p>
										To use a decision tree to classify data, we must have a set of existing data from which we can build a tree. For example, if we have a data set of basketball players that includes their height, sprint speed, free throw average, playing position, ect we could build a decision tree to help decide where each player should play <cite>(1)</cite>. Using this tree, we can then decide where new recruits should play based off the same attributes (excluding playing position of course).  Another example would be if we had existing data on video streaming users (age, gender, previous view, star ratings) then we could use a decision tree to recommend a new TV show to another user.
									</p>
									<figure class="image figure wide">
										<img src="../images/trees/fig1.svg" />
										<figcaption>
											<strong>Figure 1: </strong>
											an example of a decision tree for when to submit coursework
									</figure>
									<p>
										When making a decision tree, we wish for the tree to be as short as possible on average. This can be thought of as the average number of questions (conditions evaluated) needed to go from the start (root node) all the way to taking a decision. To understand this, we will look at the simple example below.
									</p>
									<p>
										Say you work for a large food retailer and you have a product you would like to advertise to your pregnant customers. However, the information you have from your loyalty card scheme does not include pregnancy status so you want to first be able to establish if the customer is pregnant based off their age, gender, and shopping history. You have manually collected pregnancy data on some of your customers but not all (say, from customers who have signed up to a newsletter). To do this, you could ask questions about whether they have recently bought baby formula, baby clothes, baby bottles, et cetera <cite>(2)</cite>. However, if after asking all these questions you then ask if the customer is male or female and the answer is male then all the previous questions have been wasted and the average number of questions that must be asked is far greater. This means that the trees are bigger and take more time to filter through. As such, we can see that the order in which the questions are asked makes a great difference as to how efficient the tree is.
									</p>
									<p>
										In general, the most efficient way to construct a tree is to a such that if a question has n possible outcomes, it should split the group into as close to n equal parts as is possible. To see why this is the case, and to learn about entropy, read the section on the ID3 tree-building algorithm.
									</p>

									<a href="../proc/id3/" class="button">Read about the ID3 algorithm <span class="icon fa-angle-right"></span></a>

									<br/>
									<br/>
									
									<footer class="references">
										<h2>References</h2>
										<p>1. Sayad S. Decision Tree - Classification. [Online].; 2017. Available from: http://www.saedsayad.com/decision_tree.htm.</p>
										<p>2. Hill K. Forbes. [Online].; 2017. Available from: https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured- out-a- teen-girl- was-pregnant- before-her- father-did/#3eb9073b6668.</p>
									</footer>
								</div>
							</section>

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="copyright">
							<p>
								Copyright &copy; 
								<script type="text/javascript">
								  document.write(new Date().getFullYear());
								</script>
							 	Z. Amjad, R. Padmanabhan, R. Pritchard and G. Zhelev â€“
							 	<a href="../acknowledgements/">Acknowledgements</a>
							 </p>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/skel.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../assets/js/main.js"></script>

	</body>
</html>